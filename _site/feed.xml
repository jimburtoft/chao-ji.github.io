<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-03-15T19:38:47+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">SuperComputer’s Blog</title><subtitle>Practice. Research. Learn</subtitle><entry><title type="html">SVD Explained</title><link href="http://localhost:4000/jekyll/update/2016/03/10/SVD.html" rel="alternate" type="text/html" title="SVD Explained" /><published>2016-03-10T23:42:01+08:00</published><updated>2016-03-10T23:42:01+08:00</updated><id>http://localhost:4000/jekyll/update/2016/03/10/SVD</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2016/03/10/SVD.html">&lt;p&gt;In the previous post we talked about &lt;strong&gt;Principlal Component Analysis&lt;/strong&gt;, a popular statistical technique for dimensionality reduction and feature decorrelation. Another common use case is matrix decomposition, where a matrix is factorized into a product of matrices (Eigendecomposition).&lt;/p&gt;

&lt;p&gt;A limitation of Eigendecomposition is that it only works on &lt;em&gt;square&lt;/em&gt; matrices. &lt;strong&gt;Singular Value Decomposition&lt;/strong&gt; (SVD) is a generalization of Eigendecomposition, which works on any rectangle-shaped matrix. It has been widely used in Machine Learning applications such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_semantic_analysis&quot;&gt;Latent Semantic Analysis&lt;/a&gt; (LSA), where a document-word matrix is decomposed and re-represented while keeping the pattern in the original matrix, and image compression, where a matrix holding the pixel intensities is decomposed and re-represented as the product of three much smaller matrices, from which the original image can be reconstructed.&lt;/p&gt;

&lt;p&gt;In this post, I’ll explain mathematically why a Singular Value Decomposition always exists for any rectangle-shaped matrix, and how to find it. It is assumed that you have some basic grasp of Linear Algebra (I recommend you to read the post on PCA).&lt;/p&gt;

&lt;h2 id=&quot;the-math&quot;&gt;The Math&lt;/h2&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; be any &lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt; real-valued matrix where &lt;script type=&quot;math/tex&quot;&gt;m \le n&lt;/script&gt;. It can be shown that the &lt;script type=&quot;math/tex&quot;&gt;m \times m&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;AA^{T}&lt;/script&gt; is&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;symmetric&lt;/li&gt;
  &lt;li&gt;positive semidefinite&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;\lambda_{1}, \lambda_{2}, \cdots, \lambda_{m}&lt;/script&gt; be the &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; eigenvalues of &lt;script type=&quot;math/tex&quot;&gt;AA^{T}&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;u_{1}, u_{2}, \cdots, u_{m}&lt;/script&gt; be the eigenvectors (column vectors of shape &lt;script type=&quot;math/tex&quot;&gt;m \times 1&lt;/script&gt;) corresponding to &lt;script type=&quot;math/tex&quot;&gt;\lambda_{1}, \lambda_{2}, \cdots, \lambda_{m}&lt;/script&gt;, respectively. It follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
AA^{T}u_{1} &amp;= \lambda_{1} u_{1} \\
AA^{T}u_{2} &amp;= \lambda_{2} u_{2} \\
&amp;\cdots \\
AA^{T}u_{m} &amp; = \lambda_{m} u_{m} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The eigenvalues are non-negative: &lt;script type=&quot;math/tex&quot;&gt;\lambda_{1} \ge 0, \lambda_{2} \ge 0, \cdots, \lambda_{m} \ge 0&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;The eigenvectors (corresponding to different eigenvalues) are pairwise orthogonal: &lt;script type=&quot;math/tex&quot;&gt;u_{i}^{T}u_{j} = 0&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i \ne j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;i, j = 1, \cdots, m&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ll further assume that &lt;script type=&quot;math/tex&quot;&gt;u_{1}, u_{2}, \cdots, u_{m}&lt;/script&gt; are unit vectors:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;u_{i}^{T}u_{i} = 1&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i = 1, \cdots, m&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Let’s define another &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; column vectors &lt;script type=&quot;math/tex&quot;&gt;v_{i}&lt;/script&gt; (of shape &lt;script type=&quot;math/tex&quot;&gt;n \times 1&lt;/script&gt;):&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;v_{i} = \frac{1}{\sqrt{\lambda_{i}}} A^{T}u_{i}&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i = 1, \cdots, m&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
A^{T}Av_{i} &amp; = A^{T}A \frac{1}{\sqrt{\lambda_{i}}} A^{T} u_{i} = \frac{1}{\sqrt{\lambda_{i}}} A^{T} A A^{T} u_{i} \\ 
            &amp;= \frac{1}{\sqrt{\lambda_{i}}} A^{T} \lambda_{i} u_{i} = \sqrt{\lambda_{i}} A^{T} u_{i} = \sqrt{\lambda_{i}} \sqrt{\lambda_{i}} v_{i} = \lambda_{i} v_{i}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In other words, &lt;script type=&quot;math/tex&quot;&gt;\lambda_{i}&lt;/script&gt;’s are also eigenvalues of the &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; of matrix &lt;script type=&quot;math/tex&quot;&gt;A^{T}A&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;v_{i}&lt;/script&gt;’s are the eigenvectors corresponding to &lt;script type=&quot;math/tex&quot;&gt;\lambda_{i}&lt;/script&gt;, for &lt;script type=&quot;math/tex&quot;&gt;i = 1, \cdots, m&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;and &lt;script type=&quot;math/tex&quot;&gt;v_{i}&lt;/script&gt;’s are unit vectors too:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
v_{i}^{T}v_{i} &amp; = \frac{1}{\sqrt{\lambda_{i}}} u_{i}^{T} A \cdot \frac{1}{\sqrt{\lambda_{i}}} A^{T}u_{i} \\
               &amp; = \frac{1}{\lambda_{i}} u_{i}^{T} A A^{T} u_{i} = \frac{1}{\lambda_{i}} u_{i}^{T} \lambda_{i} u_{i} = u_{i}^{T} u_{i} = 1
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that &lt;script type=&quot;math/tex&quot;&gt;A^{T} A&lt;/script&gt; is a &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; matrix and &lt;script type=&quot;math/tex&quot;&gt;n \ge m&lt;/script&gt;, so it might have additional eigenvalues &lt;script type=&quot;math/tex&quot;&gt;\lambda_{m + 1}, \lambda_{m + 2}, \cdots, \lambda_{n}&lt;/script&gt;, with the corresponding eigenvectors &lt;script type=&quot;math/tex&quot;&gt;v_{m+1}, v_{m+2}, \cdots, v_{n}&lt;/script&gt;, respectively. Because the eigenvectors &lt;script type=&quot;math/tex&quot;&gt;v_{1}, v_{2}, \cdots, v_{m}, v_{m+1}, \cdots, v_{n}&lt;/script&gt; correspond to different eigenvalues, they must be pairwise orthogonal:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;v_{i}^{T}v_{j} = 0&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i \ne j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;i, j = 1, \cdots, n&lt;/script&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Finally, let’s put together the two groups of eigenvectors into a &lt;script type=&quot;math/tex&quot;&gt;m \times m&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; and a &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U = \begin{bmatrix}u_{1}^{T}\\u_{2}^{T} \\ \vdots \\ u_{m}^{T}\end{bmatrix}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
V = \begin{bmatrix} v_{1} &amp; v_{2} &amp; \cdots &amp; v_{m} &amp; v_{m+1} &amp; \cdots&amp; v_{n} \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that both &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; are inversible because their rows/columns are pairwise orthogonal (hence linearly independent).&lt;/p&gt;

&lt;p&gt;Let’s compute the product of three matrices &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
U A V &amp; = \begin{bmatrix}u_{1}^{T}\\u_{2}^{T} \\ \vdots \\ u_{m}^{T}\end{bmatrix} \cdot A \cdot \begin{bmatrix} v_{1} &amp; v_{2} &amp; \cdots &amp; v_{m} &amp; v_{m+1} &amp; \cdots&amp; v_{n} \end{bmatrix} \\
  &amp; = \begin{bmatrix} u_{1}^{T}A \\ u_{2}^{T}A \\ \vdots \\ u_{m}^{T}A\end{bmatrix} \cdot \begin{bmatrix}v_{1} &amp; v_{2} &amp; \cdots &amp; v_{m} &amp; v_{m + 1} &amp; \cdots &amp; v_{n} \end{bmatrix} \\
 &amp; = \begin{bmatrix}
  u_{1}^{T}Av_{1} &amp; u_{1}^{T}Av_{2} &amp; \cdots &amp; u_{1}^{T}Av_{m} &amp; u_{1}^{T}Av_{m+1} &amp; \cdots &amp; u_{1}^{T}Av_{n} \\
  u_{2}^{T}Av_{1} &amp; u_{2}^{T}Av_{2} &amp; \cdots &amp; u_{2}^{T}Av_{m} &amp; u_{2}^{T}Av_{m+1} &amp; \cdots &amp; u_{2}^{T}Av_{n} \\
  \vdots &amp; \vdots &amp; &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\
  u_{m}^{T}Av_{1} &amp; u_{m}^{T}Av_{2} &amp; \cdots &amp; u_{m}^{T}Av_{m} &amp; u_{m}^{T}Av_{m+1} &amp; \cdots &amp; u_{m}^{T}Av_{n} 
  \end{bmatrix} 
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Remember that we defined &lt;script type=&quot;math/tex&quot;&gt;v_{i} = \frac{1}{\sqrt{\lambda_{i}}} A^{T}u_{i}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;i = 1, \cdots, m&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;It follows that &lt;script type=&quot;math/tex&quot;&gt;u_{i}^{T} A = \sqrt{\lambda_{i}} v_{i}^{T}&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i = 1, \cdots, m&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Therefore for &lt;script type=&quot;math/tex&quot;&gt;i = 1, \cdots, m&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;j = 1, \cdots, n&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
u_{i}^{T} A v_{j} = \sqrt{\lambda_{i}} v_{i}^{T} v_{j} = \begin{cases}
  \sqrt{\lambda_{i}}, &amp; 1 \le i = j \le m \\
  0, &amp; i \ne j
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;So&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
UAV = 
\begin{bmatrix} 
  \sqrt{\lambda_{1}} &amp;  &amp; \cdots &amp;  &amp; &amp; \cdots &amp; \\
  &amp; \sqrt{\lambda_{2}} &amp; \cdots &amp; &amp; &amp; \cdots &amp; \\
  \vdots &amp; \vdots &amp; &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\
  &amp; &amp; \cdots &amp; \sqrt{\lambda_{m}} &amp; &amp; \cdots &amp;  
\end{bmatrix}
= \Sigma %]]&gt;&lt;/script&gt;

&lt;p&gt;and the SVD of A is 
&lt;script type=&quot;math/tex&quot;&gt;A = U^{-1} \Sigma V^{-1}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recap&lt;/strong&gt;: the following are the steps to carry out SVD on any rectangle-shaped matrix &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; of shape &lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;m \le n&lt;/script&gt;)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Compute &lt;script type=&quot;math/tex&quot;&gt;A A^{T}&lt;/script&gt; and find its eigenvalues &lt;script type=&quot;math/tex&quot;&gt;\lambda_{i}&lt;/script&gt; and eigenvectors &lt;script type=&quot;math/tex&quot;&gt;u_{i}&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i = 1, \cdots, m&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Build &lt;script type=&quot;math/tex&quot;&gt;m \times m&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;U = \begin{bmatrix}u_{1}^{T}\\u_{2}^{T} \\ \vdots \\ u_{m}^{T}\end{bmatrix}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Build &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
V = \begin{bmatrix} v_{1} &amp; v_{2} &amp; \cdots &amp; v_{m} &amp; v_{m+1} &amp; \cdots&amp; v_{n} \end{bmatrix} %]]&gt;&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;v_{i} = \frac{1}{\sqrt{\lambda_{i}}} A^{T}u_{i}&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i = 1, \cdots, m&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;v_{m + 1}, \cdots, v_{n}&lt;/script&gt; are eigenvectors of &lt;script type=&quot;math/tex&quot;&gt;A^{T}A&lt;/script&gt; corresponding to eigenvalues &lt;script type=&quot;math/tex&quot;&gt;\lambda_{m + 1}, \cdots, \lambda_{n}&lt;/script&gt; that are not eigenvalues of &lt;script type=&quot;math/tex&quot;&gt;AA^{T}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Build matrix &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}
  \sqrt{\lambda_{1}} &amp;  &amp; \cdots &amp;  &amp; &amp; \cdots &amp; \\
  &amp; \sqrt{\lambda_{2}} &amp; \cdots &amp; &amp; &amp; \cdots &amp; \\
  \vdots &amp; \vdots &amp; &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\
  &amp; &amp; \cdots &amp; \sqrt{\lambda_{m}} &amp; &amp; \cdots &amp;  
\end{bmatrix} %]]&gt;&lt;/script&gt; of shape &lt;script type=&quot;math/tex&quot;&gt;n \times m&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Compute &lt;script type=&quot;math/tex&quot;&gt;A_{SVD} = U^{-1} \Sigma V^{-1}&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;h3 id=&quot;toy-example&quot;&gt;Toy example&lt;/h3&gt;
&lt;p&gt;First let’s create a small matrix that is easy to test out the theory:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# w must be &amp;gt;= h 
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'float32'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Find the eigenvalues and eigenvectors of &lt;code class=&quot;highlighter-rouge&quot;&gt;A.dot(A.T)&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;A.T.dot(A)&lt;/code&gt;, respectively:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;eigval1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvec1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eigval2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvec2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Build the matrix &lt;code class=&quot;highlighter-rouge&quot;&gt;U&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;V&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvec1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Note: we use `thres` to determine which eigenvalues of `A.T.dot(A)` 
# are NOT eigenvalues of `A.dot(A.T)`
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thres&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigval2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigval1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thres&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;V&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigvec1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigval1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
               &lt;span class=&quot;n&quot;&gt;eigvec2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Build the matrix &lt;code class=&quot;highlighter-rouge&quot;&gt;Sigma&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigval1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Finaly compute &lt;code class=&quot;highlighter-rouge&quot;&gt;A_SVD&lt;/code&gt; (Note &lt;code class=&quot;highlighter-rouge&quot;&gt;U&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;V&lt;/code&gt; are &lt;a href=&quot;https://en.wikipedia.org/wiki/Orthogonal_matrix&quot;&gt;orthonormal&lt;/a&gt; matrices so their inverses are identical to their transposed forms):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A_SVD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;You can see that the reconstructed matrix is identical to the original (up to the error in numerical precision):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;199.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;227.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;237.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;107.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;120.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;254.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;116.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;184.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;220.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;171.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;212.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;150.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;85.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;195.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;83.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;24.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;51.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;178.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;205.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;135.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_SVD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;199.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;227.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;237.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;107.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;120.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;254.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;116.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;184.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;220.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;171.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;212.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;150.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;85.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;195.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;83.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;24.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;51.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;178.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;205.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;135.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;image-compression&quot;&gt;Image compression&lt;/h3&gt;

&lt;p&gt;There is much neater way to carry out SVD than the above precedure. You can use the built-in function of NumPy in one line of code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The matrices &lt;code class=&quot;highlighter-rouge&quot;&gt;u&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;vh&lt;/code&gt; are like &lt;code class=&quot;highlighter-rouge&quot;&gt;U.T&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;V.T&lt;/code&gt; as the above example, and &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt; is a 1-D array holding the &lt;script type=&quot;math/tex&quot;&gt;\lambda_{i}&lt;/script&gt;’s (a.k.a. Singular Values, hence the name of SVD), and they are by default sorted in descending order. Usually we’d re-represent the original matrix by keeping only the largest Singular Values, and truncate &lt;code class=&quot;highlighter-rouge&quot;&gt;u&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;vh&lt;/code&gt; accordingly. This is known as &lt;strong&gt;Truncated SVD&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;truncated_svd_3_channel_compression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;`img` is 3-D array of shape [height, width, channels]
  `t` is the number of singular values to keep
  &quot;&quot;&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;u0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vh0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;u1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vh1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;u2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vh2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vh0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vh1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vh2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'uint8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'img.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;truncated_svd_3_channel_compression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are the images reconstructed using different number of SV’s as well as the original image. The total number of images is 450 (height of image).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/n10.jpg&quot; width=&quot;365&quot; /&gt; &lt;img src=&quot;/assets/n20.jpg&quot; width=&quot;365&quot; /&gt;
&lt;br /&gt;
Top 10 (L) and 20 (R) Singular Values
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/n30.jpg&quot; width=&quot;365&quot; /&gt; &lt;img src=&quot;/assets/n40.jpg&quot; width=&quot;365&quot; /&gt;
&lt;br /&gt;
Top 30 (L) and 40 (R) Singular Values
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/n50.jpg&quot; width=&quot;365&quot; /&gt; &lt;img src=&quot;/assets/fountainpark.jpg&quot; width=&quot;365&quot; /&gt;
&lt;br /&gt;
Top 50 (L) Singular Values and orignal image (R)
&lt;/p&gt;

&lt;p&gt;We can see that as we increase the number of singular values, the reconstructed image gets less blurry and contain less “outlier” pixels, and the one reconstructed from only 50 singular values offers great approximation to the orignal image.&lt;/p&gt;</content><author><name></name></author><summary type="html">In the previous post we talked about Principlal Component Analysis, a popular statistical technique for dimensionality reduction and feature decorrelation. Another common use case is matrix decomposition, where a matrix is factorized into a product of matrices (Eigendecomposition).</summary></entry><entry><title type="html">PCA Explained</title><link href="http://localhost:4000/jekyll/update/2016/02/09/PCA.html" rel="alternate" type="text/html" title="PCA Explained" /><published>2016-02-09T23:42:01+08:00</published><updated>2016-02-09T23:42:01+08:00</updated><id>http://localhost:4000/jekyll/update/2016/02/09/PCA</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2016/02/09/PCA.html">&lt;p&gt;In Machine Learning and Statistics, &lt;strong&gt;Principal Component Analysis&lt;/strong&gt; (PCA or linear PCA) is a classic and widely used technique for data transformation. PCA can, for example, transform your high dimensional data into points in two or three dimensional space so that they can be easily visualized in a scatter plot, or it could be a preprocessing step in supervised learning tasks (classification, regression) so that the data is projected in low dimensional space where the new features are decorrelated, leading to potentially better accuracy. This tutorial will explain all the math behind PCA and provide an example where you’ll practice step-by-step how to perform PCA on a synthetic data matrix.&lt;/p&gt;

&lt;h2 id=&quot;prerequisite&quot;&gt;Prerequisite&lt;/h2&gt;

&lt;p&gt;Here it is assumed that you have some basic understanding of Linear Algebra (e.g. matrix arithmetics, the geometric interpretation of inner product), Calculus (e.g. Lagrangian Multiplier) and Statistics (e.g. what is mean and variance). In addition, definitions of some math concepts and theorems (listed below) are needed to prove the properties of the resulting transformed data matrix.&lt;/p&gt;

&lt;h4 id=&quot;definition-1&quot;&gt;Definition 1&lt;/h4&gt;
&lt;p&gt;A scalar &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is called an &lt;em&gt;eigenvalue&lt;/em&gt; of a &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}&lt;/script&gt; if there is a nontrivial (non-zero) solution &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A} \mathbf{x} = \lambda \mathbf{x}&lt;/script&gt;. Such an &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; is called an &lt;em&gt;eigenvector&lt;/em&gt; corresponding to the eigenvalue &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;.&lt;/p&gt;

&lt;h4 id=&quot;definition-2&quot;&gt;Definition 2&lt;/h4&gt;
&lt;p&gt;A real-valued &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; symmetric matrix &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}&lt;/script&gt; is said to be &lt;em&gt;positive semidefinite&lt;/em&gt; if the scalar &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}^{T} \mathbf{A} \mathbf{x}&lt;/script&gt; is non-negative for every non-zero real-valued column vector &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;.&lt;/p&gt;

&lt;h4 id=&quot;theorem-1&quot;&gt;Theorem 1&lt;/h4&gt;
&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}&lt;/script&gt; is a real-valued &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; symmetric matrix, then the eigenvectors corresponding to &lt;em&gt;different&lt;/em&gt; eigenvalues must be orthogonal to each other.&lt;/p&gt;

&lt;p&gt;[http://www.math.hawaii.edu/~lee/linear/eigen.pdf]&lt;/p&gt;
&lt;h4 id=&quot;theorem-2&quot;&gt;Theorem 2&lt;/h4&gt;
&lt;p&gt;A real-valued &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt; symmetric matrix is positive semidefinite if and only if all of its eigenvalues are non-negative.&lt;/p&gt;

&lt;p&gt;[http://theanalysisofdata.com/probability/C4.html]&lt;/p&gt;
&lt;h2 id=&quot;the-math&quot;&gt;The Math&lt;/h2&gt;

&lt;p&gt;Let’s say we have a &lt;script type=&quot;math/tex&quot;&gt;n \times d&lt;/script&gt; data matrix &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is the number of observations and &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; is the number of features — each observation is described by &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; attributes.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{X} =
 \begin{pmatrix}
  x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1d} \\
  x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2d} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
  x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nd} 
 \end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now we wish to describe each observation by a single attribute. In other words, the &lt;script type=&quot;math/tex&quot;&gt;n \times d&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; will be turned into a &lt;script type=&quot;math/tex&quot;&gt;n \times 1&lt;/script&gt; column matrix (i.e. vector), and we want the variance of the sample composed of the &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; elements from this vector to be as large as possible.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;/assets/projection.pdf&quot; width=&quot;400&quot; /&gt;&lt;br /&gt;Project points from 2-D space to 1-D space&lt;/p&gt;

&lt;p&gt;One way to do this (in the case of PCA) is to project the collection of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; points in &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;-dimensional space into 1-dimensional space, or equivalently right-multiply &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; with a column vector &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol \alpha= \left( a_{1}, a_{2}, \cdots, a_{d} \right)^{T}&lt;/script&gt;

&lt;p&gt;giving rise to the &lt;script type=&quot;math/tex&quot;&gt;n \times 1&lt;/script&gt; transformed data matrix &lt;script type=&quot;math/tex&quot;&gt;\mathbf{Y}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{Y} = \mathbf{X} \boldsymbol \alpha = 
	\begin{pmatrix}
		x_{11}a_{1} + x_{12}a_{2} + \cdots + x_{1d}a_{d}\\
		x_{21}a_{1} + x_{22}a_{2} + \cdots + x_{2d}a_{d}\\
		\vdots \\
		x_{n1}a_{1} + x_{n2}a_{2} + \cdots + x_{nd}a_{d} 
	\end{pmatrix} =
	\begin{pmatrix}
		\sum_{j=1}^{d}x_{1j}a_{j} \\
		\sum_{j=1}^{d}x_{2j}a_{j} \\
		\vdots \\
		\sum_{j=1}^{d}x_{nj}a_{j}
	\end{pmatrix} =
	\begin{pmatrix}
	y_{1} \\
	y_{2} \\
	\vdots \\
	y_{n}
	\end{pmatrix}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For mathmatical convenience, the data matrix &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; is assumed to have been &lt;strong&gt;centered&lt;/strong&gt; (each column sums to one): &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^{n} x_{ij} = 0&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;j=1 \dotsc d&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;In case &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; had not been centered, you can simply subtract the column-mean from each element in each column.&lt;/p&gt;

&lt;p&gt;Given that &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; has been centered, the transformed data matrix &lt;script type=&quot;math/tex&quot;&gt;\mathbf{Y}&lt;/script&gt; will be centered as well:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathbf{\bar{Y}} &amp;=\frac{1}{n} \sum_{i=1}^{n} y_{i} = \frac{1}{n} \sum_{i=1}^{n} \left(  \sum_{j=1}^{d} x_{ij}a_{j}  \right) = \frac{1}{n} \sum_{j=1}^{d}  \left(  \sum_{i=1}^{n} x_{ij}a_{j}  \right) \\
        &amp;=  \frac{1}{n} \sum_{j=1}^{d}  a_{j}\left(  \sum_{i=1}^{n} x_{ij}  \right) =  \frac{1}{n} \sum_{j=1}^{d}  a_{j}\cdot 0 = 0
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Remember we wanted to make the variance of the sample composed of the &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; elements from &lt;script type=&quot;math/tex&quot;&gt;\mathbf{Y}&lt;/script&gt; to be as large as possible. In other words, we want to maximize &lt;script type=&quot;math/tex&quot;&gt;\textrm{Var}(\mathbf{Y})&lt;/script&gt; (over the space of all &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \alpha&lt;/script&gt;), which can be expressed in terms of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \alpha&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\textrm{Var}(\mathbf{Y}) &amp; = \frac{1}{n - 1} \sum_{i=1}^{n} \left( y_{i} - \mathbf{\bar{Y}} \right)^{2}=\frac{1}{n -1}\sum_{i=1}^{n}y_{i}^{2} \\ 
  &amp; = \frac{1}{n - 1}\mathbf{Y}^{T}\mathbf{Y}=\boldsymbol\alpha^{T}\left( \frac{1}{n-1} \mathbf{X}^{T}\mathbf{X} \right) \boldsymbol\alpha
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here we can see that &lt;script type=&quot;math/tex&quot;&gt;\textrm{Var}(\mathbf{Y})&lt;/script&gt; is proportional to the norm of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{Y}&lt;/script&gt;, which in turn depends on the norm of &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \alpha&lt;/script&gt;. If we were to allow the norm of &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \alpha&lt;/script&gt; to be unbounded, the &lt;script type=&quot;math/tex&quot;&gt;\textrm{Var}(\mathbf{Y})&lt;/script&gt; could be arbitrarily large!&lt;/p&gt;

&lt;p&gt;To fix this we need to put some constraint on &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \alpha&lt;/script&gt; — we make it a unit-length vector: &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\alpha^{T} \boldsymbol\alpha = \sum_{j=1}^{d}a_{j}^{2} = 1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So this turns the original function optimization problem into one with an equality constraint, which can be solved using &lt;a href=&quot;https://en.wikipedia.org/wiki/Lagrange_multiplier&quot;&gt;Lagrangian Multiplier&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\boldsymbol\alpha, \lambda) = \boldsymbol\alpha^{T} \left( \frac{1}{n - 1}  \mathbf{X}^{T} \mathbf{X} \right) \boldsymbol\alpha - \lambda \left(\boldsymbol\alpha^{T} \boldsymbol\alpha - 1\right)&lt;/script&gt;

&lt;p&gt;Taking the derivative of the Lagrangian function &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; with respect to &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \alpha&lt;/script&gt; and set it to zero,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial}{\partial \boldsymbol\alpha} L(\boldsymbol\alpha, \lambda) &amp; =  \frac{1}{n - 1}  \mathbf{X}^{T} \mathbf{X} \boldsymbol\alpha + \frac{1}{n - 1} \left( \mathbf{X}^{T} \mathbf{X} \right)^{T} \boldsymbol\alpha - \lambda \cdot 2 \boldsymbol \alpha \\ 
&amp; = \frac{2}{n - 1} \mathbf{X}^{T} \mathbf{X} \cdot \boldsymbol\alpha - 2 \lambda \cdot \boldsymbol\alpha = 0
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;we end up with the condition that &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \alpha&lt;/script&gt; must statisfy:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol\Sigma \boldsymbol\alpha = \lambda \boldsymbol\alpha&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\Sigma = \frac{1}{n - 1}  \mathbf{X}^{T} \mathbf{X}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;According to &lt;strong&gt;Defitition 1&lt;/strong&gt;, &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \alpha&lt;/script&gt; must be an eigenvector of &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \Sigma&lt;/script&gt; corresponding to eigenvalue &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can further show that the variance of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{Y}&lt;/script&gt; happens to be equal to &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{Var}(\mathbf{Y}) = \boldsymbol\alpha^{T}\left( \frac{1}{n-1} \mathbf{X}^{T}\mathbf{X} \right) \boldsymbol\alpha = \boldsymbol\alpha^{T} \boldsymbol\Sigma \boldsymbol\alpha = \boldsymbol\alpha^{T} \lambda \boldsymbol\alpha = \lambda \cdot 1 = \lambda&lt;/script&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;The &lt;script type=&quot;math/tex&quot;&gt;d\times d&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \Sigma&lt;/script&gt; (i.e. the covariance matrix of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;) has the following properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\Sigma = \frac{1}{n-1} \mathbf{X}^{T} \mathbf{X}&lt;/script&gt; is symmetric: &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \Sigma ^{T} = \left( \frac{1}{n-1} \mathbf{X}^{T} \mathbf{X} \right)^{T} = \frac{1}{n-1} \mathbf{X}^{T} \mathbf{X} = \boldsymbol\Sigma&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\Sigma = \frac{1}{n-1} \mathbf{X}^{T} \mathbf{X}&lt;/script&gt; is semi-positive definite: For any real-valued non-zero vector &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w} = \left(w_{1}, w_{2}, \cdots, w_{d} \right)^{T}&lt;/script&gt;,&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w}^{T} \boldsymbol\Sigma \mathbf{w} = \mathbf{w}^{T} \frac{1}{n-1} \mathbf{X}^{T} \mathbf{X} \mathbf{w} = \frac{1}{n-1} \left( \mathbf{X} \mathbf{w} \right)^{T} \mathbf{X} \mathbf{w} = \frac{1}{n-1} \Vert \mathbf{X} \mathbf{w} \Vert ^{2} \ge 0&lt;/script&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Suppose &lt;script type=&quot;math/tex&quot;&gt;\lambda_{1}, \lambda_{2}, \cdots, \lambda_{d}&lt;/script&gt; are the eigenvalues of &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \Sigma&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;\{\boldsymbol \alpha_{1i_{1}}\}, \{\boldsymbol \alpha_{2i_{2}}\}, \cdots, \{\boldsymbol \alpha_{di_{d}}\}&lt;/script&gt; are the sets of eigenvectors corresponding to each eigenvalue. Then &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; must be one of the eigenvalues, and &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \alpha&lt;/script&gt; must be an eigenvector corresponding to &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;According to &lt;strong&gt;Theorem 1&lt;/strong&gt;, we know that eigenvectors &lt;script type=&quot;math/tex&quot;&gt;\{\boldsymbol \alpha_{1i_{1}}\}, \{\boldsymbol \alpha_{2i_{2}}\}, \cdots, \{\boldsymbol \alpha_{di_{d}}\}&lt;/script&gt; are orthogonal to each other.
&lt;br /&gt;
According to &lt;strong&gt;Theorem 2&lt;/strong&gt;, we know that the eigenvalues are non-negative — &lt;script type=&quot;math/tex&quot;&gt;\lambda_{1} \ge 0, \lambda_{2} \ge 0, \cdots, \lambda_{d} \ge 0&lt;/script&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Based on the properties of &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\Sigma&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\alpha&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;\textrm{Var}(\mathbf{Y})&lt;/script&gt;, we can apply PCA on &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; in the following steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Compute the covariance matrix &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\Sigma = \frac{1}{n - 1}  \mathbf{X}^{T} \mathbf{X}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; should have been centered.&lt;/li&gt;
  &lt;li&gt;Find the eigenvalues &lt;script type=&quot;math/tex&quot;&gt;\lambda_{1}, \lambda_{2}, \cdots, \lambda_{d}&lt;/script&gt; of &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\Sigma&lt;/script&gt;, as well as the corresponding eigenvectors &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol \alpha_{1}, \boldsymbol \alpha_{2}, \cdots, \boldsymbol \alpha_{d}&lt;/script&gt;. The eigenvalues should be in descending order: &lt;script type=&quot;math/tex&quot;&gt;\lambda_{1} \ge \lambda_{2} \ge \cdots \ge \lambda_{d}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Determine the number of features &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; to keep in the transformed matrix (&lt;script type=&quot;math/tex&quot;&gt;1 \le k \le d&lt;/script&gt;).&lt;/li&gt;
  &lt;li&gt;Build the &lt;script type=&quot;math/tex&quot;&gt;d \times k&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;M = \left( \boldsymbol \alpha_{1}, \boldsymbol \alpha_{2}, \cdots, \boldsymbol \alpha_{k} \right)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Compute the transformed matrix &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}_{pca} = \mathbf{X}M = \left(\mathbf{X\alpha_{1}}, \mathbf{X\alpha_{2}}, \cdots, \mathbf{X\alpha_{k}} \right)&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;And we can prove that the columns of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}_{pca}&lt;/script&gt; have the properties that we desired:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{X\alpha_{1}}&lt;/script&gt; has the largest variance &lt;script type=&quot;math/tex&quot;&gt;\lambda_{1}&lt;/script&gt;, followed by &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X\alpha_{2}}&lt;/script&gt; with variance &lt;script type=&quot;math/tex&quot;&gt;\lambda_{2}&lt;/script&gt;, and so on.&lt;/li&gt;
  &lt;li&gt;The columns of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X\alpha_{1}}&lt;/script&gt; are orthogonal with each other — Consider &lt;script type=&quot;math/tex&quot;&gt;X\boldsymbol\alpha_{i}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;X\boldsymbol\alpha_{j}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
1 \le i &lt; j \le d %]]&gt;&lt;/script&gt;). It follows that&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \left( X\boldsymbol\alpha_{i} \right) ^{T} X\boldsymbol\alpha_{j} &amp; = \boldsymbol\alpha_{i}^{T} X^{T} X \boldsymbol\alpha_{j} = (n-1)\boldsymbol\alpha_{i}^{T} \boldsymbol\Sigma \boldsymbol\alpha_{j}  =(n-1)\boldsymbol\alpha_{i}^{T} \lambda_{j} \boldsymbol\alpha_{j} \\ &amp;=(n-1)\lambda_{j} \boldsymbol\alpha_{i}^{T}  \boldsymbol\alpha_{j} = (n-1)\lambda_{j} \cdot 0 = 0 \end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;p&gt;We conclude our discussion with an example where we’ll generate a synthetic dataset and apply PCA as prescribed above.&lt;/p&gt;

&lt;p&gt;First we will generate 2-D dataset with covariance matrix &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}0.1 &amp; 0.0 \\ 0.0 &amp; 1.0 \end{pmatrix} %]]&gt;&lt;/script&gt; and mean &lt;script type=&quot;math/tex&quot;&gt;(-1, 1)&lt;/script&gt; (shown in the figure below):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_2Ddata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;covariance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multivariate_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;covariance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;rot_mat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; 
                      &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]])&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rot_mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_2Ddata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Compute covariance matrix of a centered data matrix&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Center the data 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Compute covariance matrix
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Find the eigenvalues and eigenvectors, and sort them in descending order of eigenvalues.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# eigval[i] is the ith eigenvalue
# eigvec[:, i] is the eigenvector corresponding to eigval[i]
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;eigvec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For now we’ll keep all principle components, but you can keep the top &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt; components with largest variance:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;eigvec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigvec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Apply PCA as a linear transformation on &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigvec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Display PCA-transfomred data matrix in scatter plot&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1st Principal Component'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'2nd Principal Component'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/pca.pdf&quot; width=&quot;1200&quot; /&gt;
&lt;br /&gt;
Left: Original, uncentered. Middle: Centered. Right: PCA-transformed
&lt;/p&gt;</content><author><name></name></author><summary type="html">In Machine Learning and Statistics, Principal Component Analysis (PCA or linear PCA) is a classic and widely used technique for data transformation. PCA can, for example, transform your high dimensional data into points in two or three dimensional space so that they can be easily visualized in a scatter plot, or it could be a preprocessing step in supervised learning tasks (classification, regression) so that the data is projected in low dimensional space where the new features are decorrelated, leading to potentially better accuracy. This tutorial will explain all the math behind PCA and provide an example where you’ll practice step-by-step how to perform PCA on a synthetic data matrix.</summary></entry></feed>